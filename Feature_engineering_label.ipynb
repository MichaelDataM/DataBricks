{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f6eb528-bba0-43b8-9bc1-e27f2d728809",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = \"****\"\n",
    "storage_account_key = \"****\"\n",
    "container = \"****\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdb31810-c3c2-4c20-8c75-0f771b2f973a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as datetime\n",
    "from scipy.signal import find_peaks, peak_prominences\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy import signal\n",
    "from scipy.integrate import trapz\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "from pyspark.sql.window import Window\n",
    "import  pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StructType,StructField\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1557598-9383-4139-835b-7510a725f4a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\", storage_account_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebfaaa99-fa18-4704-864a-7a49f84ad25d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, window,expr,mean,stddev,min,max,skewness,median,lag,abs,sum,count\n",
    "\n",
    "def dataimport(filepath, sensortype, Starttime = 'NaN',Endtime = 'NaN',windows='5min'): \n",
    "\n",
    "    if sensortype == 'Glucose':\n",
    "        data = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"delimiter\", \",\").csv(f\"wasbs://{container}@{storage_account_name}.blob.core.windows.net/\"+filepath).toDF('Index','Time','Event Type','Event Subtype',\t'Patient Info','Device Info','Source Device ID','Var','Insulin Value (u)','Carb Value (grams)','Duration (hh:mm:ss)','Glucose Rate of Change (mg/dL/min)',\t'Transmitter Time (Long Integer)')\n",
    "        columns_to_drop = ['Index','Event Type','Event Subtype',\t'Patient Info','Device Info','Source Device ID','Insulin Value (u)','Carb Value (grams)','Duration (hh:mm:ss)','Glucose Rate of Change (mg/dL/min)',\t'Transmitter Time (Long Integer)']\n",
    "        data = data.select([col for col in data.columns if col not in columns_to_drop])\n",
    "\n",
    "    elif sensortype == 'Food':\n",
    "        data = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"delimiter\", \",\").csv(f\"wasbs://{container}@{storage_account_name}.blob.core.windows.net/\"+filepath).toDF('date','times','Time','time_end','logged_food','amount',\t'unit',\t'searched_food','calorie','total_carb','dietary_fiber','sugar','protein','total_fat')\n",
    "        columns_to_drop = ['date','times','time_end','logged_food','amount',\t'unit','searched_food','dietary_fiber','total_fat']\n",
    "        data = data.select([col for col in data.columns if col not in columns_to_drop])\n",
    "\n",
    "    elif sensortype == 'ACC':\n",
    "        data = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"delimiter\", \",\").csv(f\"wasbs://{container}@{storage_account_name}.blob.core.windows.net/\"+filepath).toDF('Time','x','y','z')\n",
    "        data = data.withColumn(\"Var\", (data[\"x\"]**2 + data[\"y\"]**2 + data[\"z\"]**2)**(-2))\n",
    "        columns_to_drop = ['x', 'y', 'z']\n",
    "        data = data.select([col for col in data.columns if col not in columns_to_drop])\n",
    " \n",
    "    else:\n",
    "        data = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"delimiter\", \",\").csv(f\"wasbs://{container}@{storage_account_name}.blob.core.windows.net/\"+filepath).toDF('Time','Var')\n",
    "        \n",
    "    data = data.withColumn(\"Time\", to_timestamp(data[\"Time\"], \"yyyy-MM-dd HH:mm:ss.SSS\"))\n",
    "    \n",
    "    if Starttime != 'NaN':\n",
    "        VarData = data.filter(data[\"Time\"] >= Starttime)\n",
    "        if Endttime != 'NaN':\n",
    "            VarData = VarData.filter((VarData[\"Time\"] <= Endtime))\n",
    "    else:\n",
    "        VarData = data\n",
    "\n",
    "    return VarData\n",
    "\n",
    "\n",
    "def ACC_feature(ACC_data,sensortype = 'ACC'):\n",
    "\n",
    "    Data = pd.DataFrame()\n",
    "\n",
    "    Data_1= ACC_data.withColumn(\"Sensor_Mean\", ACC_data[\"Var\"].cast(\"double\")) \\\n",
    "    .groupBy(window(\"Time\", \"5 minutes\").alias('Time')) \\\n",
    "        .agg(\n",
    "        expr(\"percentile_approx(Sensor_Mean, 0.25)\").alias(sensortype+'_Q1G'),\n",
    "        expr(\"percentile_approx(Sensor_Mean, 0.75)\").alias(sensortype+'_Q3G'),\n",
    "        mean(\"Sensor_Mean\").alias(sensortype+\"mean\"),\n",
    "        stddev(\"Sensor_Mean\").alias(sensortype+\"dev\"),\n",
    "        min(\"Sensor_Mean\").alias(sensortype+\"min\"),\n",
    "        max(\"Sensor_Mean\").alias(sensortype+\"max\"),\n",
    "        skewness(\"Sensor_Mean\").alias(sensortype+\"_skewness\")\n",
    "    )\n",
    "        \n",
    "    ACC_data = ACC_data.withColumn('Time', F.to_timestamp('Time'))\n",
    "    window_ = Window.orderBy(\"Time\")\n",
    "    ACC_data = ACC_data.withColumn(\"lag\", F.lag(\"Time\").over(window_))\n",
    "    ACC_data = ACC_data.withColumn(\"time_diff\", F.col(\"Time\").cast(\"long\") - F.col(\"lag\").cast(\"long\"))\n",
    "\n",
    "    window_spec = Window.orderBy('time_diff').rangeBetween(-7200, 0)\n",
    "    ACC_data = ACC_data.withColumn('ACC_mean_2hr', mean('Var').over(window_spec))\n",
    "    ACC_data = ACC_data.withColumn('ACC_max_2hr', max('Var').over(window_spec))\n",
    "    \n",
    "    Data_2 = ACC_data.select(['Time','ACC_mean_2hr','ACC_max_2hr'])\n",
    "\n",
    "    Data_2 = Data_2.withColumnRenamed('Time','Time_')\n",
    "    \n",
    "    Data = Data_1.join(Data_2,Data_1['Time']['start'] == Data_2['Time_'], how = 'inner')\n",
    "\n",
    "    #Data = Data.select([''])\n",
    "\n",
    "    print((sensortype + ' Import and Resample Complete'))\n",
    "    \n",
    "    return(Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5ff9f308-7425-44a4-a9f9-a05d6e72fa5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC Import and Resample Complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Time</th><th>ACC_Q1G</th><th>ACC_Q3G</th><th>ACCmean</th><th>ACCdev</th><th>ACCmin</th><th>ACCmax</th><th>ACC_skewness</th><th>Time_</th><th>ACC_mean_2hr</th><th>ACC_max_2hr</th></tr></thead><tbody><tr><td>List(2020-02-13T15:30:00.000+0000, 2020-02-13T15:35:00.000+0000)</td><td>6.120788621992582E-8</td><td>6.608468091011823E-8</td><td>6.580386701626236E-8</td><td>3.884151720395423E-8</td><td>2.5433045865057125E-9</td><td>1.6394490795313143E-6</td><td>21.894884265345883</td><td>2020-02-13T15:30:00.000+0000</td><td>1.7233871955669065E-7</td><td>1.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         [
          "2020-02-13T15:30:00.000+0000",
          "2020-02-13T15:35:00.000+0000"
         ],
         6.120788621992582e-08,
         6.608468091011823e-08,
         6.580386701626236e-08,
         3.884151720395423e-08,
         2.5433045865057125e-09,
         1.6394490795313143e-06,
         21.894884265345883,
         "2020-02-13T15:30:00.000+0000",
         1.7233871955669065e-07,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"spark.timeWindow\":true}",
         "name": "Time",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"start\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}},{\"name\":\"end\",\"type\":\"timestamp\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "ACC_Q1G",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ACC_Q3G",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ACCmean",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ACCdev",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ACCmin",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ACCmax",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ACC_skewness",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Time_",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "ACC_mean_2hr",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ACC_max_2hr",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filepath = \"ACC_001.csv\"\n",
    "# sensortype = 'ACC'\n",
    "# ACC_data = dataimport(filepath, sensortype)\n",
    "# ACC = ACC_feature(ACC_data)\n",
    "# ACC.limit(1).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef97f1b9-6e62-4e93-af24-14cbc5f37251",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, lit\n",
    "\n",
    "def exercisepts_spark(df):\n",
    "    # Define a window specification to calculate running averages\n",
    "    windowSpec = Window.orderBy(\"Time\").rowsBetween(Window.unboundedPreceding, -1)\n",
    "\n",
    "    # Calculate running averages for acc and hr\n",
    "    df = df.withColumn(\"avg_acc\", avg(\"ACC\").over(windowSpec))\n",
    "    df = df.withColumn(\"avg_hr\", avg(\"HR\").over(windowSpec))\n",
    "\n",
    "    # Define a condition for activity bouts\n",
    "    activity_condition = (col(\"ACC\") > col(\"avg_acc\")) & (col(\"HR\") > col(\"avg_hr\"))\n",
    "\n",
    "    # Apply the condition and create a new column for activity bouts\n",
    "    df = df.withColumn(\"Activity Bouts\", F.when(activity_condition, lit(1)).otherwise(lit(0)))\n",
    "\n",
    "    # Count the number of activity bouts\n",
    "    countbouts = df.filter(col(\"Activity Bouts\") == 1).count()\n",
    "\n",
    "    # Select relevant columns to return\n",
    "    returndf = df.select(\"Time\", \"Activity Bouts\")\n",
    "\n",
    "    return returndf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f5ec154-8833-4593-b027-1e43cbef238b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def HRV_feature(IBI_data, ibimultiplier = 1000,sensortype='HRV'):\n",
    "    \"\"\"\n",
    "        computes Heart Rate Variability metrics\n",
    "        Args:\n",
    "            time (pandas.DataFrame column or pandas series): time column\n",
    "            IBI (pandas.DataFrame column or pandas series): column with inter beat intervals\n",
    "            ibimultiplier (IntegerType): defualt = 1000; transforms IBI to milliseconds. If data is already in ms, set as 1\n",
    "        Returns:\n",
    "            maxHRV (FloatType): maximum HRV\n",
    "            minHRV (FloatType): minimum HRV\n",
    "            meanHRV (FloatType): mean HRV\n",
    "            medianHRV(FloatType): median HRV\n",
    "    \"\"\"\n",
    "\n",
    "    Data = pd.DataFrame()\n",
    "\n",
    "    #IBI_data['Var'] = IBI_data['Var']*ibimultiplier\n",
    "\n",
    "    Data = IBI_data.withColumn(\"Sensor_Mean\", IBI_data[\"Var\"].cast(\"double\")) \\\n",
    "    .groupBy(window(\"Time\", \"5 minutes\").alias('Time')) \\\n",
    "        .agg(\n",
    "        (mean(\"Sensor_Mean\")*ibimultiplier).alias(sensortype+\"mean\"),\n",
    "        (median(\"Sensor_Mean\")*ibimultiplier).alias(sensortype+\"dev\"),\n",
    "        (min(\"Sensor_Mean\")*ibimultiplier).alias(sensortype+\"min\"),\n",
    "        (max(\"Sensor_Mean\")*ibimultiplier).alias(sensortype+\"max\")\n",
    "    )\n",
    "    \n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1673960d-ee51-4d72-adfa-8effe10bc1dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def TEMP_feature(TEMP_data,sensortype = 'TEMP'):\n",
    "\n",
    "    Data = pd.DataFrame()\n",
    "\n",
    "    Data = TEMP_data.withColumn(\"Sensor_Mean\", col(\"Var\").cast(\"double\")) \\\n",
    "    .groupBy(window(\"Time\", \"5 minutes\").alias('Time')) \\\n",
    "        .agg(\n",
    "        expr(\"percentile_approx(Sensor_Mean, 0.25)\").alias(sensortype+'_Q1G'),\n",
    "        expr(\"percentile_approx(Sensor_Mean, 0.75)\").alias(sensortype+'_Q3G'),\n",
    "        mean(\"Sensor_Mean\").alias(sensortype+\"mean\"),\n",
    "        stddev(\"Sensor_Mean\").alias(sensortype+\"dev\"),\n",
    "        min(\"Sensor_Mean\").alias(sensortype+\"min\"),\n",
    "        max(\"Sensor_Mean\").alias(sensortype+\"max\"),\n",
    "        skewness(\"Sensor_Mean\").alias(sensortype+\"_skewness\")\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67fa7d17-ae02-41b9-b806-6253cb744396",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def HR_feature(HR_data,sensortype='HR'):\n",
    "\n",
    "    Data = pd.DataFrame()\n",
    "\n",
    "    HR_data = HR_data.withColumn('Time', F.to_timestamp(col('Time'), 'yyyy/M/d h:mm:ss a'))\n",
    "\n",
    "    Data = HR_data.withColumn(\"Sensor_Mean\", col(\"Var\").cast(\"double\")) \\\n",
    "    .groupBy(window(\"Time\", \"5 minutes\").alias('Time')) \\\n",
    "        .agg(\n",
    "        expr(\"percentile_approx(Sensor_Mean, 0.25)\").alias(sensortype+'_Q1G'),\n",
    "        expr(\"percentile_approx(Sensor_Mean, 0.75)\").alias(sensortype+'_Q3G'),\n",
    "        mean(\"Sensor_Mean\").alias(sensortype+\"mean\"),\n",
    "        stddev(\"Sensor_Mean\").alias(sensortype+\"dev\"),\n",
    "        min(\"Sensor_Mean\").alias(sensortype+\"min\"),\n",
    "        max(\"Sensor_Mean\").alias(sensortype+\"max\"),\n",
    "        skewness(\"Sensor_Mean\").alias(sensortype+\"_skewness\")\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return(Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2465bc8e-f34e-47b1-b848-3d911f7695f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filepath = 'HR_001.csv'\n",
    "# sensortype = 'HR'\n",
    "# HR_data = dataimport(filepath, sensortype)\n",
    "# HR = HR_feature(HR_data)\n",
    "# HR.limit(1).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f6c4a95-0f00-4a21-81ec-bb0b33ea9e25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def EDA_feature(EDA_data,sensortype='EDA'):\n",
    "\n",
    "    Data = pd.DataFrame()\n",
    "\n",
    "    Data = EDA_data.withColumn(\"Sensor_Mean\", col(\"Var\").cast(\"double\")) \\\n",
    "    .groupBy(window(\"Time\", \"5 minutes\").alias('Time')) \\\n",
    "        .agg(\n",
    "        expr(\"percentile_approx(Sensor_Mean, 0.25)\").alias(sensortype+'_Q1G'),\n",
    "        expr(\"percentile_approx(Sensor_Mean, 0.75)\").alias(sensortype+'_Q3G'),\n",
    "        mean(\"Sensor_Mean\").alias(sensortype+\"mean\"),\n",
    "        stddev(\"Sensor_Mean\").alias(sensortype+\"dev\"),\n",
    "        min(\"Sensor_Mean\").alias(sensortype+\"min\"),\n",
    "        max(\"Sensor_Mean\").alias(sensortype+\"max\"),\n",
    "        skewness(\"Sensor_Mean\").alias(sensortype+\"_skewness\")\n",
    "    )\n",
    "    \n",
    "    return(Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "be4eced4-94d4-40f2-baec-9198c8f05999",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filepath = 'EDA_001.csv'\n",
    "# sensortype = 'EDA'\n",
    "# EDA_data = dataimport(filepath, sensortype)\n",
    "# EDA = EDA_feature(EDA_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "371024c1-16f0-4a57-9b46-c45f56f1e3e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def NNx_feature(IBI_data, ibimultiplier=1000, x=50):\n",
    "    \"\"\"\n",
    "        computes Heart Rate Variability metrics NNx and pNNx\n",
    "        Args:\n",
    "            time (pandas.DataFrame column or pandas series): time column\n",
    "            IBI (pandas.DataFrame column or pandas series): column with inter beat intervals\n",
    "            ibimultiplier (IntegerType): defualt = 1000; transforms IBI to milliseconds. If data is already in ms, set as 1\n",
    "            x (IntegerType): default = 50; set the number of times successive heartbeat intervals exceed 'x' ms\n",
    "        Returns:\n",
    "            NNx (FloatType): the number of times successive heartbeat intervals exceed x ms\n",
    "            pNNx (FloatType): the proportion of NNx divided by the total number of NN (R-R) intervals. \n",
    "    \"\"\"\n",
    "    Data = pd.DataFrame()\n",
    "\n",
    "    window_spec = Window().orderBy(\"Time\")\n",
    "    IBI_data = IBI_data.withColumn(\"diff_IBI_mean\", abs(col(\"Var\") - lag(\"Var\").over(window_spec)))\n",
    "\n",
    "    Data = IBI_data.withColumn(\"Sensor_Mean\", col(\"Var\").cast(\"double\")) \\\n",
    "     .groupBy(window(\"Time\", \"5 minutes\").alias('Time')) \\\n",
    "         .agg(\n",
    "         (mean(\"Sensor_Mean\")*1000).alias(\"IBI_mean\"),\n",
    "         sum((col(\"diff_IBI_mean\") > 0.05).cast(\"int\")).alias('NN50'),\n",
    "         (sum((col(\"diff_IBI_mean\") > 0.05).cast(\"int\"))/count(col('diff_IBI_mean')) * 100).alias('pNN50')\n",
    "\n",
    "     )\n",
    "    \n",
    "    \n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64095884-4519-41eb-8278-cc0454aab72b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def Food_feature(Food):\n",
    "    Data = pd.DataFrame()\n",
    "\n",
    "    Food = Food.groupby('Time').agg(sum(\"calorie\").alias(\"calorie\"),\n",
    "        sum(\"protein\").alias(\"protein\"),\n",
    "        sum(\"sugar\").alias(\"sugar\"),\n",
    "        sum(\"total_carb\").alias(\"total_carb\"))\n",
    "\n",
    "    Food = Food.withColumn('Time', F.to_timestamp('Time'))\n",
    "    window = Window.orderBy(\"Time\")\n",
    "    Food = Food.withColumn(\"lag\", F.lag(\"Time\").over(window))\n",
    "    Food = Food.withColumn(\"time_diff\", F.col(\"Time\").cast(\"long\") - F.col(\"lag\").cast(\"long\"))\n",
    "\n",
    "    for window in ['2H','8H','24H']:\n",
    "        window_spec = Window.orderBy('time_diff').rangeBetween(-3600*int(window[0]), 0)\n",
    "        # Calculate rolling sum for each column\n",
    "        Food = Food.withColumn(f'Calories_{window}', sum('calorie').over(window_spec))\n",
    "        Food = Food.withColumn(f'Protein_{window}', sum('protein').over(window_spec))\n",
    "        Food = Food.withColumn(f'Sugar_{window}', sum('sugar').over(window_spec))\n",
    "        Food = Food.withColumn(f'Carbs_{window}', sum('total_carb').over(window_spec))\n",
    "    Food = Food.select(['Time','Calories_2H','Protein_2H','Sugar_2H','Carbs_2H','Calories_8H','Protein_8H','Sugar_8H','Carbs_8H','Calories_24H','Protein_24H','Sugar_24H','Carbs_24H'])\n",
    "    \n",
    "    return Food\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47ef023b-055c-45e7-a204-80f9b9a10876",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def PeaksEDA(eda):\n",
    "\n",
    "    eda_pd = eda.toPandas()\n",
    "    \n",
    "    EDAy = eda_pd['Var'].to_numpy()\n",
    "    EDAx = eda_pd['Time'].to_numpy()\n",
    "    peaks, _ = find_peaks(EDAy, height=0, distance=4, prominence=0.3)\n",
    "    \n",
    "    peaks_x = [eda_pd['Time'][i] for i in peaks]\n",
    "    \n",
    "    peakdf_pd = pd.DataFrame({\"Time\": peaks_x, \"Peak\": [1] * len(peaks_x)})\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"Time\", TimestampType(), True),\n",
    "        StructField(\"Peak\", DoubleType(), True),\n",
    "    ])\n",
    "    peakdf = spark.createDataFrame(peakdf_pd, schema=schema)\n",
    "\n",
    "    return peakdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5145bad5-9f85-4b32-9cea-ded1d6f06d22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def Calculate_PeakEda(data):\n",
    "\n",
    "    data = data.withColumn('Time', F.to_timestamp('Time'))\n",
    "    window = Window.orderBy(\"Time\")\n",
    "    data = data.withColumn(\"lag\", F.lag(\"Time\").over(window))\n",
    "    data = data.withColumn(\"time_diff\", F.col(\"Time\").cast(\"long\") - F.col(\"lag\").cast(\"long\"))\n",
    "\n",
    "    windowSpec = Window.orderBy(\"time_diff\").rangeBetween(-3600 * 2, 0)\n",
    "\n",
    "    data = data.withColumn(\"PeakEDA2hr_sum\", sum(\"Peak\").over(windowSpec))\n",
    "    data = data.withColumn(\"PeakEDA2hr_mean\", F.avg(\"Peak\").over(windowSpec))\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "901002b9-9310-4ee5-8390-ba76faa78965",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def Glucose_feature(Dexcom):\n",
    "\n",
    "    Data = pd.DataFrame()\n",
    "\n",
    "    Data = Dexcom.withColumn(\"Sensor_Mean\", col(\"Var\").cast(\"double\")) \\\n",
    "     .groupBy(window(\"Time\", \"5 minutes\").alias('Time')) \\\n",
    "         .agg(\n",
    "         (mean(\"Sensor_Mean\")).alias(\"Glucose_mean\")\n",
    "\n",
    "     )\n",
    "         \n",
    "    # Convert the timestamp to time and extract hour and minute\n",
    "    Data = Data.withColumn('Timefrommidnight', F.date_format(F.col('Time')['start'], 'HH:mm:ss'))\n",
    "    Data = Data.withColumn('minfrommid', (F.hour(col('Time')['start']) * 60 + F.minute(col('Time')['start']) + F.second(col('Time')['start']) / 60).cast('int'))\n",
    "    Data = Data.withColumn('hourfrommid', (F.col('minfrommid') / 60).cast('int'))\n",
    "    \n",
    "    Data = Data.select(['Time','Glucose_mean','minfrommid','hourfrommid'])\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74cf75f7-f282-4378-bb1c-0554f474c1a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merge_patient(label,id):\n",
    "\n",
    "    postfix = '00'+str(id) if id < 10 else '0'+str(id)\n",
    "\n",
    "    ## ACC\n",
    "    filepath = \"ACC_\"+postfix+\".csv\"\n",
    "    sensortype = 'ACC'\n",
    "    ACC_data = dataimport(filepath, sensortype)\n",
    "    ACC = ACC_feature(ACC_data)\n",
    "\n",
    "    ## IBI\n",
    "    filepath = 'IBI_'+postfix+\".csv\"\n",
    "    sensortype = 'IBI'\n",
    "    IBI_data = dataimport(filepath, sensortype)\n",
    "    HRV = HRV_feature(IBI_data)\n",
    "\n",
    "    ## TEMP\n",
    "    filepath = 'TEMP_'+postfix+\".csv\"\n",
    "    sensortype = 'TEMP'\n",
    "    TEMP_data = dataimport(filepath, sensortype)\n",
    "    TEMP = TEMP_feature(TEMP_data)\n",
    "\n",
    "    ## HR\n",
    "    filepath = 'HR_'+postfix+\".csv\"\n",
    "    sensortype = 'HR'\n",
    "    HR_data = dataimport(filepath, sensortype)\n",
    "    HR = HR_feature(HR_data)\n",
    "\n",
    "    ## EDA\n",
    "    filepath = 'EDA_'+postfix+\".csv\"\n",
    "    sensortype = 'EDA'\n",
    "    EDA_data = dataimport(filepath, sensortype)\n",
    "    EDA = EDA_feature(EDA_data)\n",
    "    \n",
    "    ## NN\n",
    "    NNx = NNx_feature(IBI_data)\n",
    "\n",
    "    ## Food\n",
    "    filepath = 'Food_Log_'+postfix+\".csv\"\n",
    "    sensortype = 'Food'\n",
    "    Food_data = dataimport(filepath, sensortype)\n",
    "    Food = Food_feature(Food_data)\n",
    "\n",
    "    ## PeakEDA\n",
    "    Peak_EDA = Calculate_PeakEda(PeaksEDA(EDA_data))\n",
    "    Peak_EDA = Peak_EDA.select(['Time','Peak','PeakEDA2hr_sum','PeakEDA2hr_mean'])\n",
    "\n",
    "    # ## Activity\n",
    "    # ACC_data = ACC_data.withColumnRenamed('Var','ACC')\n",
    "    # HR_data = HR_data.withColumnRenamed('Var','HR')\n",
    "    # data_test = ACC_data.join(HR_data,on = 'Time',how = 'inner')\n",
    "    # Activity = exercisepts_spark(data_test)\n",
    "\n",
    "    ## Glucose\n",
    "    filepath = 'Dexcom_'+postfix+\".csv\"\n",
    "    sensortype = 'Glucose'\n",
    "    glucose_data = dataimport(filepath, sensortype)\n",
    "    Glucose = Glucose_feature(glucose_data)\n",
    "\n",
    "    ## Join the features\n",
    "\n",
    "    result_df_label = ACC.join(HR, on=\"Time\", how=\"full_outer\")\\\n",
    "    .join(NNx,on=\"Time\", how=\"full_outer\")\\\n",
    "        .join(EDA,on=\"Time\", how=\"full_outer\")\\\n",
    "            .join(HRV,on=\"Time\", how=\"full_outer\")\\\n",
    "                .join(Glucose,on == 'Time', how=\"inner\")\\\n",
    "                    .join(TEMP,on=\"Time\", how=\"full_outer\")\n",
    "    result_df_label = result_df_label.withColumn('Time', col('Time')['start'])\n",
    "    result_df_label = result_df_label.withColumn('PatientID', lit(id))\n",
    "    final_data_label = result_df_label.join(Food,on = 'Time',how = 'left').join(Peak_EDA,on = 'Time',how = 'left')\n",
    "\n",
    "\n",
    "    result_df_unlabel = ACC.join(HR, on=\"Time\", how=\"full_outer\")\\\n",
    "    .join(NNx,on=\"Time\", how=\"full_outer\")\\\n",
    "        .join(EDA,on=\"Time\", how=\"full_outer\")\\\n",
    "            .join(HRV,on=\"Time\", how=\"full_outer\")\\\n",
    "                    .join(TEMP,on=\"Time\", how=\"full_outer\")\n",
    "    result_df_label = result_df_label.withColumn('Time', col('Time')['start'])\n",
    "    result_df_label = result_df_label.withColumn('PatientID', lit(id))\n",
    "    final_data_label = result_df_label.join(Food,on = 'Time',how = 'left').join(Peak_EDA,on = 'Time',how = 'left')\n",
    "    \n",
    "    ## Add a column - patientid\n",
    "    if label == 1:\n",
    "        return result_data_label\n",
    "    else:\n",
    "        return result_data_unlabel\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write data into different files as per the patient id\n",
    "patient_id = list(range(2,17))\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "demo_df = spark.read.option('header','true').csv(f\"wasbs://{container}@{storage_account_name}.blob.core.windows.net/Demographics.csv\")\n",
    "demo_df = demo_df.withColumn(\"Gender\",when(demo_df['Gender']=='Male',1).otherwise(0))\n",
    "demo_df = demo_df.withColumn(\"HbA1c\",demo_df['HbA1c'])\n",
    "demo_df = demo_df.withColumn(\"ID\",demo_df['ID'].cast(\"string\")) \n",
    "\n",
    "final_data = merge_patient(1,1)\n",
    "final_data.limit(1).display()\n",
    "for id in patient_id:\n",
    "    final_data_ = merge_patient(1,id)\n",
    "    final_data = final_data.union(final_data_)\n",
    "    final_data.limit(1).display()\n",
    "\n",
    "final_data_ = final_data.join(demo_df,data.PatientID == demo_df.ID,how = 'left')\n",
    "\n",
    "final_data.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").option(\"delimiter\", \",\").csv(f\"wasbs://{container}@{storage_account_name}.#blob.core.windows.net/feature_eng/final_merged.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f9191e9-1a1e-407e-897a-16cfb93bfbb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2023-11-28 20_52_48 (1)",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
